# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aRL5Z4KK1xCS_Hd_hd84AbiaXUNoH7KK
"""

import random
import json
import pandas as pd
from pathlib import Path
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
from torch.utils.data import Dataset
from transformers import AutoModel, AutoTokenizer

device = "cuda" if torch.cuda.is_available() else "cpu"

class Devign:

    def __init__(self, data_dir:str, seed:int):

        self._seed = seed

        self._df = pd.read_json(data_dir)  # legge il file JSON in un DataFrame pandas

        self._X = self._df['func'].values  # estrae la colonna 'func'
        self._y = self._df['target'].values

        self._X_train, self._X_test, self._y_train, self._y_test = train_test_split(self._X,
                                                                                    self._y,
                                                                                    test_size=0.2,
                                                                                    stratify=self._y,
                                                                                    random_state = self._seed)
    # vari getter
    @property
    def dataframe(self) -> pd.DataFrame:
        return self._df

    @property
    def train_data(self):
        return self._X_train, self._y_train
    @property
    def test_data(self):
        return self._X_test, self._y_test

# Seed
seed = 42
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)

# Dataset
dataset = Devign(data_dir='/content/embedded.json', seed = seed)
X_train, y_train = dataset.train_data
X_test, y_test = dataset.test_data

print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)

class CWEDataset(Dataset):

    def __init__(self, X, y, tokenizer, model):

        self._tokenizer = tokenizer
        self._X = X
        self._y = y
        self._model = model

    def __len__(self):
        return len(self._X)    # ritorna il numero di esempi

    def __getitem__(self, idx):
        embedding = torch.tensor(self._X[idx], dtype=torch.float)
        label = torch.tensor(self._y[idx], dtype=torch.float)

        return embedding, label

checkpoint = "Salesforce/codet5p-110m-embedding"

tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)
model = AutoModel.from_pretrained(checkpoint, trust_remote_code=True).to(device)

train_data = CWEDataset(X_train, y_train, tokenizer, model)
test_data = CWEDataset(X_test, y_test, tokenizer, model)

# print(train_data[0])
# print(train_data[0:2])

# DataLoader
batch_size = 16
train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
test_loader  = DataLoader(test_data, batch_size=batch_size, shuffle=False)  # restituisce batch di (batch_x, batch_y) e mescola i dati a ogni epoca (shuffle) per migliorare generalizzazione

# Definisci l'architettura (stessa della versione skorch)
class NeuralNetMLP(nn.Module):
    def __init__(self, num_features=256, num_hidden1=128, num_hidden2=64, num_classes=1):
        super().__init__()
        self.fc1 = nn.Linear(num_features, num_hidden1)
        self.fc2 = nn.Linear(num_hidden1, num_hidden2)
        self.fc3 = nn.Linear(num_hidden2, num_classes)
    def forward(self, x):
        x = F.relu(self.fc1(x))  # ottengo vettore di 128
        x = F.relu(self.fc2(x))
        x = F.sigmoid(self.fc3(x))  # ottieni valori tra 0 e 1
        return x

model_clf = NeuralNetMLP().to(device) # crea l'istanza del modello e la sposta sul device scelto

# Loss, ottimizzatore e parametri di training
loss_fn = nn.MSELoss()           # dice mediamente quanto è l'errore commesso (es: 0.4 avrà un errore di 0.4 o 0.6)
optimizer = torch.optim.SGD(model_clf.parameters(), lr=1e-3)  #stocastic gradient descent
max_epochs = 10

# Training loop esplicito (forward -> loss -> backward -> step)
model_clf.train()  # mette il modello in modalità training

for epoch in range(max_epochs):
    running_loss = 0.0  # accumulator per loss totale dell'epoca

    for embeddings, labels in train_loader:
        embeddings = embeddings.to(device)
        labels = labels.to(device)

        outputs = model_clf(embeddings).squeeze()

        loss = loss_fn(outputs, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    running_loss/= len(train_loader)
    print(f"Epoch [{epoch+1}/{max_epochs}], Loss: {running_loss:.4f}")

# Valutazione su test (no_grad per non tracciare operazioni)
model_clf.eval()            # mette il modello in modalità evaluation

all_preds = []
all_labels = []

with torch.no_grad():   # disabilita il tracking del grafo e la memorizzazione dei gradienti — riduce memoria e velocizza il forward durante la valutazione.

    for emdebbings, labels in test_loader:
        emdebbings = emdebbings.to(device)

        probs = model_clf(emdebbings).squeeze()
        predictions = (probs >= 0.5).int()

        all_preds.extend(predictions.cpu().detach().numpy())
        all_labels.extend(labels.cpu().detach().numpy())

acc = accuracy_score(all_labels, all_preds)
report = classification_report(all_labels, all_preds)

print(f"\nTest Accuracy: {acc:.4f}\n")
print("Classification Report:\n", report)